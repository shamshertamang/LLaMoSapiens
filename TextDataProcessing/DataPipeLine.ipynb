{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# installation",
   "id": "eb83245a149313c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T02:59:30.542545Z",
     "start_time": "2025-08-18T02:59:29.476426Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install tiktoken",
   "id": "2aa2cce382fa586",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\shams\\desktop\\projects\\myprojects\\llamosapiens\\.venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shams\\desktop\\projects\\myprojects\\llamosapiens\\.venv\\lib\\site-packages (from tiktoken) (2025.7.34)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shams\\desktop\\projects\\myprojects\\llamosapiens\\.venv\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shams\\desktop\\projects\\myprojects\\llamosapiens\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shams\\desktop\\projects\\myprojects\\llamosapiens\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shams\\desktop\\projects\\myprojects\\llamosapiens\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shams\\desktop\\projects\\myprojects\\llamosapiens\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# byte pair encoding",
   "id": "77e461144bed15d7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-18T03:42:06.380031Z",
     "start_time": "2025-08-18T03:42:06.347418Z"
    }
   },
   "source": [
    "import tiktoken\n",
    "\n",
    "print(tiktoken.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T03:42:09.151234Z",
     "start_time": "2025-08-18T03:42:08.867643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "print(tokenizer)\n",
    "# dir(tiktoken)"
   ],
   "id": "d5ebb2b5e44471b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'gpt2'>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T03:10:22.626282Z",
     "start_time": "2025-08-18T03:10:22.622284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    " )\n",
    "integers = tokenizer.encode(test, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ],
   "id": "c3d39bb1a76e5547",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T03:10:24.834592Z",
     "start_time": "2025-08-18T03:10:24.829507Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(integers)",
   "id": "1e0a13e2ebf57c53",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T03:11:01.880442Z",
     "start_time": "2025-08-18T03:11:01.875509Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(tokenizer.encode(test, allowed_special={\"<|endoftext|>\"}))",
   "id": "de396b7f102f7aeb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Sampling with Sliding Window",
   "id": "a7b6a89f34ecdead"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T03:42:23.954745Z",
     "start_time": "2025-08-18T03:42:23.932718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"./../BookAndDataFiles/txtfiles/test\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ],
   "id": "f83138d57be3efbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:50:10.940226Z",
     "start_time": "2025-08-18T11:50:10.933708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enc_sample = enc_text[:50]\n",
    "print(tokenizer.decode(enc_sample))"
   ],
   "id": "acb7f78586bf739f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like a rag doll.\n",
      "\n",
      "Today our big brains pay off nicely, because we can produce cars and guns that enable us to move much faster than chimps, and shoot them from a safe distance instead of wrestling. But cars and guns are a\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:51:51.697846Z",
     "start_time": "2025-08-18T11:51:51.691387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_size = 4\n",
    "for i in range(1, context_size+1):\n",
    "    x = enc_sample[:i]\n",
    "    y = enc_sample[i]\n",
    "    print(f'{x} --> {y}')"
   ],
   "id": "4650328e20ae3af1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2339] --> 257\n",
      "[2339, 257] --> 34232\n",
      "[2339, 257, 34232] --> 3654\n",
      "[2339, 257, 34232, 3654] --> 13\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:51:38.328172Z",
     "start_time": "2025-08-18T11:51:38.323113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1, context_size+1):\n",
    "    x = enc_text[:i]\n",
    "    y = enc_text[i]\n",
    "    print(f'{tokenizer.decode(x)} --> {tokenizer.decode([y])}')"
   ],
   "id": "60e40cca13a42e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like -->  a\n",
      "like a -->  rag\n",
      "like a rag -->  doll\n",
      "like a rag doll --> .\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T15:42:39.213051Z",
     "start_time": "2025-08-18T15:42:39.206893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class GPT_Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, txt, max_len, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        encoded_text = tokenizer.encode(txt)\n",
    "        for i in range(0, len(encoded_text)-max_len, stride):\n",
    "            input_chunk = encoded_text[i : i+max_len]\n",
    "            target_chunk = encoded_text[i+1 : i+max_len+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ],
   "id": "6ddeb6161250e142",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T15:42:44.017727Z",
     "start_time": "2025-08-18T15:42:44.013381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_data_loader(txt, max_len = 256, stride = 128, batch_size= 8,\n",
    "                       shuffle=True, num_workers=0, drop_last=True):\n",
    "\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "    dataset = GPT_Dataset(tokenizer, txt, max_len, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ],
   "id": "9fb674f99ea65e9",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T16:26:01.323014Z",
     "start_time": "2025-08-18T16:26:01.312834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"./../BookAndDataFiles/txtfiles/test\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_data_loader(txt = raw_text, max_len = 5, stride = 2, batch_size = 1, shuffle = False)\n",
    "data_iter = iter(dataloader)"
   ],
   "id": "145ec1f69fd13ce2",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T16:26:03.495613Z",
     "start_time": "2025-08-18T16:26:03.489609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_batch = next(data_iter)\n",
    "print(\"Hello\")\n",
    "print(first_batch)"
   ],
   "id": "b791ccbfb26031a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "[tensor([[ 2339,   257, 34232,  3654,    13]]), tensor([[  257, 34232,  3654,    13,   198]])]\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T16:26:08.044071Z",
     "start_time": "2025-08-18T16:26:08.039081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ],
   "id": "3e40d7ff6569ce40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[34232,  3654,    13,   198,   198]]), tensor([[3654,   13,  198,  198, 8888]])]\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T16:36:50.134260Z",
     "start_time": "2025-08-18T16:36:50.123775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloader = create_data_loader(txt = raw_text, max_len = 5, stride = 2, batch_size = 8, shuffle = False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f'inputs: \\n{inputs}\\n')\n",
    "print(f'targets: \\n{targets}')"
   ],
   "id": "e65d4551857e15dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[ 2339,   257, 34232,  3654,    13],\n",
      "        [34232,  3654,    13,   198,   198],\n",
      "        [   13,   198,   198,  8888,   674],\n",
      "        [  198,  8888,   674,  1263, 14290],\n",
      "        [  674,  1263, 14290,  1414,   572],\n",
      "        [14290,  1414,   572, 16576,    11],\n",
      "        [  572, 16576,    11,   780,   356],\n",
      "        [   11,   780,   356,   460,  4439]])\n",
      "\n",
      "targets: \n",
      "tensor([[  257, 34232,  3654,    13,   198],\n",
      "        [ 3654,    13,   198,   198,  8888],\n",
      "        [  198,   198,  8888,   674,  1263],\n",
      "        [ 8888,   674,  1263, 14290,  1414],\n",
      "        [ 1263, 14290,  1414,   572, 16576],\n",
      "        [ 1414,   572, 16576,    11,   780],\n",
      "        [16576,    11,   780,   356,   460],\n",
      "        [  780,   356,   460,  4439,  5006]])\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# creating token embeddings",
   "id": "fea916753f570765"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T17:20:08.741085Z",
     "start_time": "2025-08-18T17:20:08.737895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3"
   ],
   "id": "4b3f3609c6e576c6",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T17:21:52.119049Z",
     "start_time": "2025-08-18T17:21:52.100320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ],
   "id": "5fdf5590fdc2de98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T17:23:47.359024Z",
     "start_time": "2025-08-18T17:23:47.352872Z"
    }
   },
   "cell_type": "code",
   "source": "print(embedding_layer(torch.tensor([3])))",
   "id": "77159c4947cc335d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "972a65ba00e8b64a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# positional embedding",
   "id": "e4eb1ed71b86a22e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T17:31:44.622181Z",
     "start_time": "2025-08-18T17:31:44.446342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "with open(\"./../BookAndDataFiles/txtfiles/Homo Deus\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_data_loader(txt = raw_text, max_len = max_length, batch_size = 8, shuffle = False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f'inputs: \\n{inputs}')\n",
    "print(f'inputs shape: {inputs.shape}')"
   ],
   "id": "51e108dc41ad3126",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[   35,   276,  3299,   198],\n",
      "        [10465,   406,  4629,  6779],\n",
      "        [  290, 27714,   663,  2951],\n",
      "        [ 9862,    11, 23684,   290],\n",
      "        [  262,   938,  1178,  4647],\n",
      "        [  674, 23162,   995,   447],\n",
      "        [ 8208,    12, 11085,  4289],\n",
      "        [  389,   356,  1016,   284]])\n",
      "inputs shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T17:33:04.869568Z",
     "start_time": "2025-08-18T17:33:04.861488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "token_embeddings.shape"
   ],
   "id": "fe97466eaadc6f7e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T17:39:07.650334Z",
     "start_time": "2025-08-18T17:39:07.645854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings"
   ],
   "id": "c56571e3cb04913a",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fa15734b235d1f0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Summarise",
   "id": "22b6282fbc8be645"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T18:01:40.677752Z",
     "start_time": "2025-08-18T18:01:40.671310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "txt_files_path = \"./../BookAndDataFiles/txtfiles/\"\n",
    "\n",
    "class GPT_Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, txt_files, max_len, stride):\n",
    "        # text_files is a list of text file_names\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for txt in txt_files:\n",
    "            with open(Path(os.path.join(txt_files_path, txt)).resolve(), \"r\", encoding = \"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            encoded_text = tokenizer.encode(content, allowed_special={\"<|endoftext|>\"})\n",
    "            for i in range(0, len(encoded_text)-max_len, stride):\n",
    "                input_chunk = encoded_text[i : i+max_len]\n",
    "                target_chunk = encoded_text[i+1 : i+max_len+1]\n",
    "                self.input_ids.append(torch.tensor(input_chunk))\n",
    "                self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_data_loader(txt_files, max_len = 256, stride = 128, batch_size= 8,\n",
    "                       shuffle=True, num_workers=0, drop_last=True):\n",
    "\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "    dataset = GPT_Dataset(tokenizer, txt_files, max_len, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ],
   "id": "81a04098cd56a883",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T19:16:48.189420Z",
     "start_time": "2025-08-18T19:16:42.111026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test\n",
    "txt_files = []\n",
    "max_length = 4\n",
    "\n",
    "for txt_file_name in os.listdir(txt_files_path):\n",
    "    if txt_file_name == \"test\":\n",
    "        continue\n",
    "    txt_files.append(txt_file_name)\n",
    "\n",
    "dataloader = create_data_loader(txt_files = txt_files, max_len = max_length, batch_size = 8, shuffle = False, stride = 1, num_workers = 0)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f'inputs: \\n{inputs}')"
   ],
   "id": "f7c6a23b129874c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[15842,   198,   198, 27245],\n",
      "        [  198,   198, 27245,  7994],\n",
      "        [  198, 27245,  7994,   262],\n",
      "        [27245,  7994,   262,  4897],\n",
      "        [ 7994,   262,  4897,  7994],\n",
      "        [  262,  4897,  7994,   262],\n",
      "        [ 4897,  7994,   262,  6434],\n",
      "        [ 7994,   262,  6434,  4418]])\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T19:16:23.907908Z",
     "start_time": "2025-08-18T19:16:23.903464Z"
    }
   },
   "cell_type": "code",
   "source": "len(dataloader)",
   "id": "a73c6a56e57c6e37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537184"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T19:16:48.295740Z",
     "start_time": "2025-08-18T19:16:48.204678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = max_length\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "print(input_embeddings.shape)"
   ],
   "id": "ae20c17b4193c386",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4c63a6cd2c2d643f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
